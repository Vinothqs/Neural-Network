{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dca7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "sql_folder = './sql'  # Folder containing your .sql files\n",
    "output_folder = './csv'  # Folder to save extracted CSVs\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3774975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows_from_sql(sql_text):\n",
    "    insert_statements = re.findall(\n",
    "        r\"INSERT INTO [`\\\"]?([\\w_]+)[`\\\"]?.*?VALUES\\s*(.*?);\", \n",
    "        sql_text, re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    extracted_data = {}\n",
    "    for table_name, values_block in insert_statements:\n",
    "        rows = re.findall(r\"\\((.*?)\\)\", values_block, re.DOTALL)\n",
    "        parsed_rows = []\n",
    "        for row in rows:\n",
    "            # Split by comma but ignore commas inside quotes\n",
    "            values = re.split(r\",(?=(?:[^']*'[^']*')*[^']*$)\", row)\n",
    "            cleaned = [v.strip().strip(\"'\").strip('\"') for v in values]\n",
    "            parsed_rows.append(cleaned)\n",
    "        if table_name in extracted_data:\n",
    "            extracted_data[table_name].extend(parsed_rows)\n",
    "        else:\n",
    "            extracted_data[table_name] = parsed_rows\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a691f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No valid data found in 01_mysql_create.sql\n",
      "‚úÖ Extracted 9235 rows from 02_mysql_populate_author.sql ‚Üí author.csv\n",
      "‚úÖ Extracted 568 rows from 03_mysql_populate_publisher.sql ‚Üí publisher.csv\n",
      "‚úÖ Extracted 27 rows from 04_mysql_populate_lookups.sql ‚Üí book_language.csv\n",
      "‚úÖ Extracted 4 rows from 04_mysql_populate_lookups.sql ‚Üí shipping_method.csv\n",
      "‚úÖ Extracted 2 rows from 04_mysql_populate_lookups.sql ‚Üí address_status.csv\n",
      "‚úÖ Extracted 6 rows from 04_mysql_populate_lookups.sql ‚Üí order_status.csv\n",
      "‚úÖ Extracted 37 rows from 05_mysql_populate_book.sql ‚Üí book.csv\n",
      "‚úÖ Extracted 17642 rows from 06_mysql_populate_bookauthor.sql ‚Üí book_author.csv\n",
      "‚úÖ Extracted 232 rows from 07_mysql_populate_country.sql ‚Üí country.csv\n",
      "‚úÖ Extracted 1000 rows from 08_mysql_populate_address.sql ‚Üí address.csv\n",
      "‚úÖ Extracted 2000 rows from 09_mysql_populate_customer.sql ‚Üí customer.csv\n",
      "‚ö†Ô∏è No valid data found in 10_mysql_populate_others.sql\n",
      "‚ö†Ô∏è No valid data found in 11_mysql_populate_order.sql\n",
      "‚ö†Ô∏è No valid data found in 12_mysql_populate_orderline.sql\n",
      "‚ö†Ô∏è No valid data found in 13_mysql_populate_orderhistory.sql\n"
     ]
    }
   ],
   "source": [
    "for filename in sorted(os.listdir(sql_folder)):\n",
    "    if filename.endswith(\".sql\"):\n",
    "        path = os.path.join(sql_folder, filename)\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        tables = extract_rows_from_sql(content)\n",
    "        if not tables:\n",
    "            print(f\"‚ö†Ô∏è No valid data found in {filename}\")\n",
    "        for table, rows in tables.items():\n",
    "            df = pd.DataFrame(rows)\n",
    "            csv_path = os.path.join(output_folder, f\"{table}.csv\")\n",
    "            df.to_csv(csv_path, index=False, header=False)\n",
    "            print(f\"‚úÖ Extracted {len(rows)} rows from {filename} ‚Üí {table}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e07f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sql_folder = './sql'\n",
    "output_folder = './csv_output'\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1251622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows_from_sql(sql_text):\n",
    "    insert_statements = re.findall(\n",
    "        r'INSERT INTO [`\"]?([\\w_]+)[`\"]?.*?VALUES\\s*(.*?);',\n",
    "        sql_text, re.DOTALL | re.IGNORECASE\n",
    "    )\n",
    "    extracted_data = {}\n",
    "    for table_name, values_block in insert_statements:\n",
    "        rows = re.findall(r\"\\((.*?)\\)\", values_block, re.DOTALL)\n",
    "        parsed_rows = []\n",
    "        for row in rows:\n",
    "            values = re.split(r\",(?=(?:[^']*'[^']*')*[^']*$)\", row)\n",
    "            cleaned = [v.strip().strip(\"'\").strip('\"') for v in values]\n",
    "            parsed_rows.append(cleaned)\n",
    "        if table_name in extracted_data:\n",
    "            extracted_data[table_name].extend(parsed_rows)\n",
    "        else:\n",
    "            extracted_data[table_name] = parsed_rows\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9ad6550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ customer_address.csv created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_customer_address():\n",
    "    customer_ids = list(range(1, 2001))\n",
    "    address_ids = random.choices(range(1, 1001), k=2000)\n",
    "    status_ids = [1] * 2000\n",
    "\n",
    "    customer_ids += random.choices(range(1, 2001), k=750)\n",
    "    address_ids += random.choices(range(1, 1001), k=750)\n",
    "    status_ids += [1] * 750\n",
    "\n",
    "    customer_ids += random.choices(range(1, 2001), k=400)\n",
    "    address_ids += random.choices(range(1, 1001), k=400)\n",
    "    status_ids += [2] * 400\n",
    "\n",
    "    customer_ids += random.choices(range(1, 2001), k=200)\n",
    "    address_ids += random.choices(range(1, 1001), k=200)\n",
    "    status_ids += [1] * 200\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'customer_id': customer_ids,\n",
    "        'address_id': address_ids,\n",
    "        'status_id': status_ids\n",
    "    })\n",
    "    df.to_csv(f\"{output_folder}/customer_address.csv\", index=False)\n",
    "    print(\"‚úÖ customer_address.csv created\")\n",
    "generate_customer_address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82d9c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ cust_order.csv created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_cust_order():\n",
    "    num_orders = 7550\n",
    "    data = {\n",
    "        \"order_date\": [datetime.now() - timedelta(days=random.randint(0, 1095)) for _ in range(num_orders)],\n",
    "        \"customer_id\": [random.randint(1, 2000) for _ in range(num_orders)],\n",
    "        \"shipping_method_id\": [random.randint(1, 4) for _ in range(num_orders)],\n",
    "        \"dest_address_id\": [random.randint(1, 1000) for _ in range(num_orders)],\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"order_date\"] = df[\"order_date\"].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df.to_csv(f\"{output_folder}/cust_order.csv\", index=False)\n",
    "    print(\"‚úÖ cust_order.csv created\")\n",
    "generate_cust_order()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef1209c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ order_line.csv created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_order_line():\n",
    "    record_count = 4000 + 2000 + 1000 + 300 + 500 + 50\n",
    "    order_ids = [random.randint(1000, 9999) for _ in range(record_count)]\n",
    "    book_ids = [random.randint(1, 11126) for _ in range(record_count)]\n",
    "    prices = [round(random.uniform(0, 20), 2) for _ in range(record_count)]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'order_id': order_ids,\n",
    "        'book_id': book_ids,\n",
    "        'price': prices\n",
    "    })\n",
    "    df.to_csv(f\"{output_folder}/order_line.csv\", index=False)\n",
    "    print(\"‚úÖ order_line.csv created\")\n",
    "generate_order_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a101afb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ order_history.csv created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_order_history():\n",
    "    record_count = 7547 + 6800 + 4000 + 3500 + 300 + 200\n",
    "    order_ids = list(range(1001, 1001 + record_count))\n",
    "    status_ids = [1]*7547 + [2]*6800 + [3]*4000 + [4]*3500 + [5]*300 + [6]*200\n",
    "    random.shuffle(status_ids)\n",
    "    status_dates = [datetime.now() - timedelta(days=random.randint(1, 30)) for _ in range(len(status_ids))]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'order_id': random.choices(order_ids, k=len(status_ids)),\n",
    "        'status_id': status_ids,\n",
    "        'status_date': [d.strftime('%Y-%m-%d %H:%M:%S') for d in status_dates]\n",
    "    })\n",
    "    df.to_csv(f\"{output_folder}/order_history.csv\", index=False)\n",
    "    print(\"‚úÖ order_history.csv created\")\n",
    "generate_order_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfebb18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ All SQL files processed and missing ones generated as CSV.\n"
     ]
    }
   ],
   "source": [
    "print('üéâ All SQL files processed and missing ones generated as CSV.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db47f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: address.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: ADDRESS\n",
      "Shape: (999, 5)\n",
      "Missing values:\n",
      " 1                      0\n",
      "57                     0\n",
      "Glacier Hill Avenue    0\n",
      "Torbat-e JƒÅm           0\n",
      "95                     0\n",
      "dtype: int64\n",
      "Loading: address_status.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: ADDRESS_STATUS\n",
      "Shape: (1, 2)\n",
      "Missing values:\n",
      " 1         0\n",
      "Active    0\n",
      "dtype: int64\n",
      "Loading: author.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: AUTHOR\n",
      "Shape: (9234, 2)\n",
      "Missing values:\n",
      " A. Bartlett Giamatti    0\n",
      "1                       2\n",
      "dtype: int64\n",
      "Loading: book.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: BOOK\n",
      "Shape: (36, 7)\n",
      "Missing values:\n",
      " The World''s First Love: Mary  Mother of God     0\n",
      "1                                               10\n",
      "8987059752                                      10\n",
      "2                                               10\n",
      "276                                             10\n",
      "1996-09-01                                      10\n",
      "1010                                            10\n",
      "dtype: int64\n",
      "Loading: book_author.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: BOOK_AUTHOR\n",
      "Shape: (17641, 2)\n",
      "Missing values:\n",
      " 1570    0\n",
      "2823    0\n",
      "dtype: int64\n",
      "Loading: book_language.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: BOOK_LANGUAGE\n",
      "Shape: (26, 3)\n",
      "Missing values:\n",
      " eng        0\n",
      "1          0\n",
      "English    0\n",
      "dtype: int64\n",
      "Loading: churn_model_data.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: CHURN_MODEL_DATA\n",
      "Shape: (1959, 5)\n",
      "Missing values:\n",
      " customer_id           0\n",
      "order_count           0\n",
      "shipping_variation    0\n",
      "address_count         0\n",
      "churn                 0\n",
      "dtype: int64\n",
      "Loading: country.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: COUNTRY\n",
      "Shape: (231, 2)\n",
      "Missing values:\n",
      " 1              0\n",
      "Afghanistan    2\n",
      "dtype: int64\n",
      "Loading: cust_order.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: CUST_ORDER\n",
      "Shape: (7550, 4)\n",
      "Missing values:\n",
      " order_date            0\n",
      "customer_id           0\n",
      "shipping_method_id    0\n",
      "dest_address_id       0\n",
      "dtype: int64\n",
      "Loading: customer.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: CUSTOMER\n",
      "Shape: (1999, 4)\n",
      "Missing values:\n",
      " 1                     0\n",
      "Ursola                0\n",
      "Purdy                 0\n",
      "upurdy0@cdbaby.com    0\n",
      "dtype: int64\n",
      "Loading: customer_address.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: CUSTOMER_ADDRESS\n",
      "Shape: (3350, 3)\n",
      "Missing values:\n",
      " customer_id    0\n",
      "address_id     0\n",
      "status_id      0\n",
      "dtype: int64\n",
      "Loading: order_history.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: ORDER_HISTORY\n",
      "Shape: (22347, 3)\n",
      "Missing values:\n",
      " order_id       0\n",
      "status_id      0\n",
      "status_date    0\n",
      "dtype: int64\n",
      "Loading: order_line.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: ORDER_LINE\n",
      "Shape: (7850, 3)\n",
      "Missing values:\n",
      " order_id    0\n",
      "book_id     0\n",
      "price       0\n",
      "dtype: int64\n",
      "Loading: order_status.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: ORDER_STATUS\n",
      "Shape: (5, 2)\n",
      "Missing values:\n",
      " 1                 0\n",
      "Order Received    0\n",
      "dtype: int64\n",
      "Loading: publisher.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: PUBLISHER\n",
      "Shape: (567, 2)\n",
      "Missing values:\n",
      " 10/18     0\n",
      "1        36\n",
      "dtype: int64\n",
      "Loading: shipping_method.csv\n",
      "\n",
      "========================================\n",
      "DATAFRAME: SHIPPING_METHOD\n",
      "Shape: (3, 3)\n",
      "Missing values:\n",
      " 1           0\n",
      "Standard    0\n",
      "5.9         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load CSVs and check structure\n",
    "folder_path = '.'\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.csv') and not f.startswith('cleaned_')]\n",
    "\n",
    "for file in sorted(files):\n",
    "    print(f\"Loading: {file}\")\n",
    "    df = pd.read_csv(file)\n",
    "    print(f\"\\n{'='*40}\\nDATAFRAME: {file.upper().replace('.CSV','')}\\nShape: {df.shape}\")\n",
    "    print(\"Missing values:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0c03f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning: address.csv\n",
      "Cleaning: address_status.csv\n",
      "Cleaning: author.csv\n",
      "Cleaning: book.csv\n",
      "Cleaning: book_author.csv\n",
      "Cleaning: book_language.csv\n",
      "Cleaning: churn_model_data.csv\n",
      "Cleaning: country.csv\n",
      "Cleaning: cust_order.csv\n",
      "Cleaning: customer.csv\n",
      "Cleaning: customer_address.csv\n",
      "Cleaning: order_history.csv\n",
      "Cleaning: order_line.csv\n",
      "Cleaning: order_status.csv\n",
      "Cleaning: publisher.csv\n",
      "Cleaning: shipping_method.csv\n",
      "‚úÖ Cleaned files saved with 'cleaned_' prefix.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files = [f for f in os.listdir('.') if f.endswith('.csv') and not f.startswith('cleaned_')]\n",
    "\n",
    "for file in sorted(files):\n",
    "    print(f\"Cleaning: {file}\")\n",
    "    df = pd.read_csv(file)\n",
    "    df_cleaned = df.dropna()\n",
    "    df_cleaned.to_csv(f\"cleaned_{file}\", index=False)\n",
    "\n",
    "print(\"‚úÖ Cleaned files saved with 'cleaned_' prefix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87912fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load cleaned CSV files\n",
    "cust_order = pd.read_csv(\"cleaned_cust_order.csv\")\n",
    "customer = pd.read_csv(\"cleaned_customer.csv\")\n",
    "customer_address = pd.read_csv(\"cleaned_customer_address.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de9e178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature 1: Order Count per Customer ---\n",
    "order_count = cust_order['customer_id'].value_counts().rename(\"order_count\")\n",
    "\n",
    "# --- Feature 2: Shipping Method Variation per Customer ---\n",
    "shipping_variation = cust_order.groupby('customer_id')['shipping_method_id'].nunique().rename(\"shipping_variation\")\n",
    "\n",
    "# --- Feature 3: Address Count per Customer ---\n",
    "address_count = customer_address.groupby('customer_id')['address_id'].nunique().rename(\"address_count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786c8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', 'Ursola', 'Purdy', 'upurdy0@cdbaby.com']\n"
     ]
    }
   ],
   "source": [
    "print(customer.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50356b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id first_name last_name                   email\n",
      "0            1     Ursola     Purdy      upurdy0@cdbaby.com\n",
      "1            2   Ruthanne    Vatini       rvatini1@fema.gov\n",
      "2            3     Reidar   Turbitt  rturbitt2@geocities.jp\n",
      "3            4       Rich     Kirsz      rkirsz3@jalbum.net\n",
      "4            5    Carline     Kupis        ckupis4@tamu.edu\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the CSV without assuming the first row is the header\n",
    "customer = pd.read_csv(\"cleaned_customer.csv\", header=None)\n",
    "\n",
    "# Step 2: Assign proper column names manually\n",
    "customer.columns = ['customer_id', 'first_name', 'last_name', 'email']\n",
    "\n",
    "# Step 3: Confirm it's fixed\n",
    "print(customer.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e37cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Combine All Features ---\n",
    "features_df = pd.DataFrame({\n",
    "    'customer_id': customer['customer_id']\n",
    "}).set_index('customer_id')\n",
    "\n",
    "features_df = features_df.join([order_count, shipping_variation, address_count])\n",
    "features_df.fillna(0, inplace=True)\n",
    "features_df = features_df.astype(int)\n",
    "\n",
    "# --- Target Label: Churn ---\n",
    "features_df['churn'] = features_df['order_count'].apply(lambda x: 1 if x < 5 else 0)\n",
    "\n",
    "features_df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2df8889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature engineering completed. Sample output:\n",
      "   customer_id  order_count  shipping_variation  address_count  churn\n",
      "0            1            4                   3              2      1\n",
      "1            2            6                   3              1      0\n",
      "2            3            0                   0              1      1\n",
      "3            4            3                   2              1      1\n",
      "4            5            3                   2              2      1\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Feature engineering completed. Sample output:\")\n",
    "print(features_df.head())\n",
    "\n",
    "# Optional: Save for next step\n",
    "features_df.to_csv(\"features_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74f99264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "churn\n",
      "1    1355\n",
      "0     645\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['churn'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c69458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['order_count', 'shipping_variation', 'address_count'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f8ad245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 'days_since_last_order' added and saved to features_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_11304\\2714666683.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['days_since_last_order'].fillna(df['days_since_last_order'].max(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your feature dataset\n",
    "df = pd.read_csv(\"features_dataset.csv\")\n",
    "\n",
    "# Load the order data to calculate recency\n",
    "cust_order = pd.read_csv(\"cleaned_cust_order.csv\")\n",
    "cust_order['order_date'] = pd.to_datetime(cust_order['order_date'])\n",
    "\n",
    "# Get most recent order date per customer\n",
    "last_order = cust_order.groupby('customer_id')['order_date'].max()\n",
    "\n",
    "# Calculate days since last order\n",
    "days_since_last_order = (pd.Timestamp.today() - last_order).dt.days.rename(\"days_since_last_order\")\n",
    "if 'days_since_last_order' in df.columns:\n",
    "    df.drop(columns=['days_since_last_order'], inplace=True)\n",
    "# Merge into your dataset\n",
    "df = df.set_index('customer_id').join(days_since_last_order).reset_index()\n",
    "\n",
    "# Fill missing values (for customers with no orders at all)\n",
    "df['days_since_last_order'].fillna(df['days_since_last_order'].max(), inplace=True)\n",
    "df['days_since_last_order'] = df['days_since_last_order'].astype(int)\n",
    "\n",
    "# Save updated feature set\n",
    "df.to_csv(\"features_dataset.csv\", index=False)\n",
    "print(\"‚úÖ 'days_since_last_order' added and saved to features_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3283d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Model Evaluation Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.9525</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.759494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient Boosting</th>\n",
       "      <td>0.9425</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.722892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.689655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Logistic Regression</th>\n",
       "      <td>0.9350</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.628571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Accuracy  Precision    Recall  F1 Score\n",
       "Random Forest          0.9525   0.857143  0.681818  0.759494\n",
       "Gradient Boosting      0.9425   0.769231  0.681818  0.722892\n",
       "Decision Tree          0.9325   0.697674  0.681818  0.689655\n",
       "Logistic Regression    0.9350   0.846154  0.500000  0.628571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best model 'Random Forest' saved as 'best_churn_model.pkl'\n",
      "üìÑ Saved predictions to 'churn_prediction_results.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load feature dataset\n",
    "df = pd.read_csv(\"features_dataset.csv\")\n",
    "\n",
    "# Step 2: Define smart, fuzzy churn logic\n",
    "def smart_churn(row):\n",
    "    if row['days_since_last_order'] > 300 and row['order_count'] < 2 and row['shipping_variation'] <= 1:\n",
    "        return 1\n",
    "    elif row['days_since_last_order'] > 500 and row['order_count'] <= 1:\n",
    "        return 1\n",
    "    elif random.random() < 0.03:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['churn'] = df.apply(smart_churn, axis=1)\n",
    "\n",
    "# Step 3: Define features (X) and label (y)\n",
    "X = df[['order_count', 'shipping_variation', 'address_count', 'days_since_last_order']]\n",
    "y = df['churn']\n",
    "\n",
    "# Step 4: Train-test split (with stratification)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 5: Initialize models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Step 6: Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    results[name] = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1 Score\": f1_score(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "# Step 7: Display all model scores\n",
    "results_df = pd.DataFrame(results).T\n",
    "results_df_sorted = results_df.sort_values(by=\"F1 Score\", ascending=False)\n",
    "print(\"üìä Model Evaluation Results:\")\n",
    "display(results_df_sorted)\n",
    "\n",
    "# Step 8: Auto-select best model using tie-breaker logic\n",
    "f1_scores = results_df[\"F1 Score\"]\n",
    "max_f1 = f1_scores.max()\n",
    "top_models = [name for name, score in f1_scores.items() if score == max_f1]\n",
    "\n",
    "# Tie-breaker preference\n",
    "model_priority = [\"Gradient Boosting\", \"Random Forest\", \"Logistic Regression\", \"Decision Tree\"]\n",
    "for model_name in model_priority:\n",
    "    if model_name in top_models:\n",
    "        best_model_name = model_name\n",
    "        break\n",
    "\n",
    "best_model = models[best_model_name]\n",
    "joblib.dump(best_model, \"best_churn_model.pkl\")\n",
    "print(f\"‚úÖ Best model '{best_model_name}' saved as 'best_churn_model.pkl'\")\n",
    "\n",
    "# Step 9: Save predictions for entire dataset\n",
    "df['predicted_churn'] = best_model.predict(X)\n",
    "df.to_excel(\"churn_prediction_results.xlsx\", index=False)\n",
    "print(\"üìÑ Saved predictions to 'churn_prediction_results.xlsx'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
